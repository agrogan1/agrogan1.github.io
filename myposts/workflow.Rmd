---
title: "Workflow"
description: |
  Thoughts about data science / data analysis workflow.
author:
  - name: Andy Grogan-Kaylor 
    url: https://agrogan1.github.io/
    affiliation: University of Michigan
    affiliation_url: https://umich.edu/
date: "`r Sys.Date()`"
output:
  distill::distill_article:
    highlight: haddock
    toc: yes
---

```{css, echo=FALSE}

d-article blockquote {
  color: black;
  border-left: 2px solid #FFCB05; 
  padding: 0.5em 10px;
}
  
```

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE)

```

# Introduction

I have increasingly been thinking about the idea of *workflow* in data science / data analysis work.

So many workflows follow the same conceptual pattern.

# Visually and Conceptually

```{r}

DiagrammeR::grViz("

digraph workflow {

node [fontname = Helvetica]

ask [label = 'ask a question']

open [label = 'open the raw data']

clean [label = 'clean the data, e.g. outliers & errors']

wrangle [label = 'create any new variables or scales']

descriptives [label = 'descriptive statistics']

visualize [label = 'visualize the data']

analyze [label = 'analyze with bivariate or multivariate statistics']

share [label = 'share your results with your community(ies)']

ask -> open 

open -> clean 

clean -> wrangle

wrangle -> descriptives

descriptives -> visualize

visualize -> analyze

analyze -> share


}")

```

# Write A Script

Increasingly, we want to think about workflows that are 

* **documentable**: We have a record of what we did if we want to double check our work, clarify a result, or develop a new project with a similar process.
* **auditable**: Others can double check our work.
* **replicable**: Others can replicate our findings with the same or new data.
* **scalable**: We are developing a process that can be as easily used with *thousands* or *millions* of rows of data as it can with *ten* rows of data. We are developing a process that can be easily repeated if we are constantly getting new data, e.g. getting new data every week, or every month.

> Always (or usually) beginning with the raw data, and then writing and running a script that generates our results allows us to develop a process that does all of these things.

Below is an example that uses the [Palmer Penguins](https://allisonhorst.github.io/palmerpenguins/) data set. 

The example below is in Stata, due to Stata's ease of readability, but could as easily be written in any other language that has scripting, such as SPSS, SAS, R, or Julia.

```{stata, eval = FALSE, echo = TRUE}

* Open Data

use "https://github.com/agrogan1/Stata/raw/master/do-files/penguins.dta", clear 

* Clean and Wrangle Data

generate big_penguin = body_mass_g > 4000 // create a big penguin variable

* Visualize

graph bar body_mass_g, over(species) scheme(s1color) // bar graph

twoway scatter culmen_length_mm body_mass_g, scheme(s1color) // scatterplot

* Analyze

regress culmen_length_mm body_mass_g // regress culmen length on body mass


```



